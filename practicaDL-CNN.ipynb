{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=12, color=blue>  <center> <b>Aprendizaje Profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=blue>  <center> <b> (Una concisa introducción)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6, color=green>  <center> <b>Ejemplo práctico en laboratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Práctica sobre aprendizaje profundo en la que se mostrarán algunos ejemplos de este tipo de técnicas, como los autocodificadores, la técnica de \"drop-out\" o las redes convolucionales (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Magic para representaciones on-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se importan algunas librerías básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metodosDL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d4977cd607ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmetodosDL\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metodosDL'"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#import os\n",
    "#import time\n",
    "#import scipy.io as sio\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import metodosDL as mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> A continuación se cargan los datos de la base de datos MNIST (números manuscritos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train-images-idx3-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d1a66240dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0metiquetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mxTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcarga_MNIST_patrones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0myTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcarga_MNIST_etiquetas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-labels-idx1-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mxTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcarga_MNIST_patrones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3d1a66240dba>\u001b[0m in \u001b[0;36mcarga_MNIST_patrones\u001b[0;34m(fichero)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcarga_MNIST_patrones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfichero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Lectura de los datos MNIST con el formato de Yann LeCun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfichero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mpatrones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Los patrones son vectores que contienen imágenes 2D en escala de grises\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfm/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfm/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-images-idx3-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "def carga_MNIST_patrones(fichero):\n",
    "        # Lectura de los datos MNIST con el formato de Yann LeCun\n",
    "        with gzip.open(fichero, 'rb') as f:\n",
    "            patrones = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # Los patrones son vectores que contienen imágenes 2D en escala de grises\n",
    "        # patrones = patrones.reshape(-1, 28, 28)\n",
    "        # Se agrupan los patrones por filas (cada patrón es una fila, cada columna un pixel)\n",
    "        patrones = patrones.reshape(-1,784)\n",
    "        # Conversión de bytes a float32 en el rango [0,1].\n",
    "        return patrones / np.float32(256)\n",
    "    \n",
    "def carga_MNIST_etiquetas(fichero):\n",
    "        # Lectura de las etiquetas en el formato de Yann LeCun\n",
    "        with gzip.open(fichero, 'rb') as f:\n",
    "            etiquetas = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # Las etiquetas son valores enteros\n",
    "        return etiquetas\n",
    "\n",
    "xTrain = carga_MNIST_patrones('train-images-idx3-ubyte.gz')\n",
    "yTrain = carga_MNIST_etiquetas('train-labels-idx1-ubyte.gz')\n",
    "xTest = carga_MNIST_patrones('t10k-images-idx3-ubyte.gz')\n",
    "yTest = carga_MNIST_etiquetas('t10k-labels-idx1-ubyte.gz') \n",
    "\n",
    "# Los datos de entrenamiento se dividen en conjuntos de entrenamiento y de validación\n",
    "xTrain, xVal = xTrain[:-10000], xTrain[-10000:]\n",
    "yTrain, yVal = yTrain[:-10000], yTrain[-10000:]\n",
    "\n",
    "# Se transponen los datos para tener los patrones por columnas\n",
    "xTrain=xTrain.T\n",
    "xTest=xTest.T\n",
    "xVal=xVal.T\n",
    "\n",
    "print(\"Las imágenes se agrupan en una matriz, donde cada columna es una imagen con 784 píxeles (28x28)\\r\\n\")\n",
    "print(\"Tamaño de la matriz de patrones de entrenamiento: %s x %s\"%(xTrain.shape))\n",
    "print(\"Tamaño de la matriz de patrones de validación: %s x %s\"%(xVal.shape))\n",
    "print(\"Tamaño de la matriz de patrones de test: %s x %s\"%(xTest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representa un dígito (ejemplo de los datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indice=0;\n",
    "imagen = xTrain[:,indice]\n",
    "imagen=imagen.reshape(28,28)\n",
    "etiqueta=yTrain[indice]\n",
    "fig, eje = plt.subplots(figsize=(6, 6))\n",
    "eje.imshow(imagen, cmap='gray')\n",
    "eje.set_title('{label}'.format(label=etiqueta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representan ahora ejemplos de cada clase (el primero de cada clase en el conjunto de entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[]\n",
    "# Se busca la posición en la que se encuentra el primer patrón de cada clase\n",
    "for k in range(10):\n",
    "    indNum = [i for (i, val) in enumerate(yTrain[0:30]) if val == int(k)]\n",
    "    indices.append(indNum[0])\n",
    "\n",
    "fig, ejes = plt.subplots(1, 10, figsize=(12, 1.5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejes.flat, indices):\n",
    "    imagen = xTrain[:,k]\n",
    "    imagen=imagen.reshape(28,28)\n",
    "    etiqueta = yTrain[k]\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    eje.set_title('{label}'.format(label=etiqueta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=4, color=blue> Por cuestiones de carga computacional se va a utilizar una menor resolución diezmando\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indDown=[]\n",
    "#pixeles=np.linspace(0,27,num=28)  # para mantener 28 x 28\n",
    "#pixeles=np.linspace(0,26,num=14)  # para diezmar a 14 x 14\n",
    "pixeles=np.linspace(0,27,num=10)  # para diezmar a 10 x 10\n",
    "for ka in pixeles:\n",
    "    for kb in pixeles:\n",
    "        indDown.append(int(ka)*28+int(kb))       \n",
    "\n",
    "xTrainD=xTrain[indDown,:]\n",
    "xValD=xVal[indDown,:]\n",
    "xTestD=xTest[indDown,:]\n",
    "\n",
    "fig, ejes = plt.subplots(1, 10, figsize=(12, 1.5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejes.flat, indices):\n",
    "    imagen = xTrainD[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    etiqueta = yTrain[k]\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    eje.set_title('{label}'.format(label=etiqueta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Para reducir la carga computacional se realiza un muestreo, reduciendo el número de imágenes de entrenamiento por un factor \"*factorS*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factorS=25    # Factor de muestreo (el número de muestras se divide por este factor)\n",
    "indTrainS=range(0,len(yTrain),factorS)\n",
    "xTrainS=xTrainD[:,indTrainS]\n",
    "yTrainS=yTrain[indTrainS]\n",
    "\n",
    "print(\"Tamaño de la matriz de patrones de entrenamiento: %s x %s\"%(xTrainS.shape))\n",
    "print(\"Tamaño de la matriz de patrones de validación: %s x %s\"%(xValD.shape))\n",
    "print(\"Tamaño de la matriz de patrones de test: %s x %s\"%(xTestD.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=4, color=blue>Habitualmente, para una clasificación multiclase se utiliza una red con M salidas, una por cada clase. La referencia de la salida para el patrón de la clase *i*-ésima es un vector con M-1 ceros y un uno en la posición *i*-ésima. Se crea por tanto una matriz con los vectores de referencia para cada patrón\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTrainS=np.zeros((10,len(yTrainS)))\n",
    "for k in range(len(yTrainS)):\n",
    "    aux=yTrainS[k]\n",
    "    eTrainS[aux,k]=1\n",
    "print(\"Tamaño de la matriz de etiquetas de referencia: %s x %s\"%(eTrainS.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se muestra la referencia de la salida para un cierto patrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice=0\n",
    "print(\"Patrón de indice %d - Clase = %d\"%(indice,yTrainS[indice]))\n",
    "print(eTrainS[:,indice])\n",
    "imagen = xTrainS[:,indice]\n",
    "imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "etiqueta=yTrainS[indice]\n",
    "fig, eje = plt.subplots(figsize=(3, 3))\n",
    "eje.imshow(imagen, cmap='gray')\n",
    "eje.set_title('{label}'.format(label=etiqueta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=6, color=green>Entrenamiento de una red neuronal no profunda \n",
    "\n",
    "<font size=4, color=blue>En primer lugar se definen los parámetros de la red, que es un perceptrón multicapa (MLP) de una única capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fCoste='entropia'   # Tipo de función de coste ('mmse', 'entropia' ,'wmmse')\n",
    "opt='gradiente'     # Tipo de optimizador ('gradiente', 'momento')\n",
    "tAct=[1,4]          # Tipos de activación de las neuronas de la capa oculta y de la capa de salida\n",
    "paso=[1e-3]         # Parametro de paso de la actualización por gradiente\n",
    "Nn=125              # Número de neuronas de la capa oculta \n",
    "Niter=250           # Número de iteraciones para la actualización por gradiente\n",
    "Nbatch=0            # Número de muestras del mini-batch\n",
    "Nep=[Niter,Nbatch]  # Parámetros agrupados\n",
    "\n",
    "(Ne,Np)=xTrainS.shape\n",
    "(Ns,Nada)=eTrainS.shape\n",
    "\n",
    "# Inicialización de los pesos de la capa oculta y de la capa de salida\n",
    "inicializacionExterna=False\n",
    "if inicializacionExterna:\n",
    "    woini = 0.2*np.random.rand(Nn,Ne+1)-0.1\n",
    "    wsini = 0.2*np.random.rand(Ns,Nn+1)-0.1\n",
    "    W=[woini,wsini]\n",
    "else:\n",
    "    W=mdl.entrena_mlp(xTrainS,eTrainS,[Nn,[]],[0,0])[0]\n",
    "    woini=W[0]\n",
    "    wsini=W[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se realiza el entrenamiento del MLP y se representa la evolución del error cuadrático entre la salida de la red y los vectores de referencia para cada patrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,Nep,fCoste=fCoste,optimizador=opt,tAct=tAct)[0:3]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del coste a minimizar', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Coste : %s' % (fCoste), fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se obtiene la salida de la red para los patrones de entrenamiento, los de validación y los de test y se estiman la probabilidad de acierto obtenidas sobre los 3 conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(eTrain,xOculta)=mdl.mlp(xTrainS,W,tAct)\n",
    "(eVal,xOculta)=mdl.mlp(xValD,W,tAct)\n",
    "(eTest,xOculta)=mdl.mlp(xTestD,W,tAct)\n",
    "\n",
    "yTrainE=eTrain.argmax(axis=0)\n",
    "yValE=eVal.argmax(axis=0)\n",
    "yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "diferencia=yTrainE-yTrainS\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "diferencia=yValE-yVal\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "diferencia=yTestE-yTest\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "\n",
    "print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "print(\"Probabilidad de acierto (Test): %s\"%(PaTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se pueden ver algunos de los ejemplos de patrones clasificados erróneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indiceFallo=0\n",
    "indice=indFallo[indiceFallo];\n",
    "imagen = xTestD[:,indice]\n",
    "imagen=imagen.reshape(10,10)\n",
    "etiqueta=yTest[indice]\n",
    "fig, eje = plt.subplots(figsize=(3, 3))\n",
    "#eje.title('{label}'.format(label=etiqueta))\n",
    "eje.imshow(imagen, cmap='gray')\n",
    "print(\"La etiqueta correcta es %s y la red predice %s\"%(etiqueta,yTestE[indice]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se pueden representar los coeficientes de una neurona específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indice=0\n",
    "imagen = W[0][indice,0:Ne]\n",
    "resI=int(np.sqrt(Ne))\n",
    "imagen=imagen.reshape(resI,resI)\n",
    "fig, eje = plt.subplots(figsize=(6, 6))\n",
    "eje.set_title('Pesos de la neurona {label}'.format(label=indice+1))\n",
    "eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representan los pesos obtenidos en algunas de las neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices=range(100)\n",
    "resI=int(np.sqrt(Ne))\n",
    "\n",
    "fig, ejes = plt.subplots(5, 20, figsize=(19, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejes.flat, indices):\n",
    "    imagen = W[0][k,0:Ne]\n",
    "    imagen=imagen.reshape(resI,resI)\n",
    "    eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representa la evolución de la tasa de acierto con las iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[woini,wsini]\n",
    "NiterPaso=50\n",
    "NrepBucle=60\n",
    "evoPa=np.zeros((3,NrepBucle))\n",
    "for kbucle in range(NrepBucle):\n",
    "    (W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,[NiterPaso,Nbatch],fCoste=fCoste,optimizador=opt,tAct=tAct)[0:3]\n",
    "    (eTrain,xOculta)=mdl.mlp(xTrainS,W,tAct)\n",
    "    (eVal,xOculta)=mdl.mlp(xValD,W,tAct)\n",
    "    (eTest,xOculta)=mdl.mlp(xTestD,W,tAct)\n",
    "\n",
    "    yTrainE=eTrain.argmax(axis=0)\n",
    "    yValE=eVal.argmax(axis=0)\n",
    "    yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "    diferencia=yTrainE-yTrainS\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "    diferencia=yValE-yVal\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "    diferencia=yTestE-yTest\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "    PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "    evoPa[:,kbucle]=[PaTrain,PaVal,PaTest]\n",
    "    \n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "ejeIter=NiterPaso*np.linspace(1,NrepBucle,NrepBucle)\n",
    "plt.plot(ejeIter,evoPa[0,:],ejeIter,evoPa[1,:],ejeIter,evoPa[2,:])\n",
    "plt.title('Evolucion de la Prob. Acierto', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Prob. Acierto (%)', fontdict=font)\n",
    "\n",
    "print(\"Entrenamiento (azul) - Validación (Naranja) - Test (verde)\")\n",
    "print(\" \")\n",
    "print(\"Max. Pa (Entrenamiento): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[0,:]),NiterPaso*np.argmax(evoPa[0,:])))\n",
    "print(\"Max. Pa (Validación): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[1,:]),NiterPaso*np.argmax(evoPa[1,:])))\n",
    "print(\"Max. Pa (Test) : %3.2f (Iter. %3.2f)\"%(np.max(evoPa[2,:]),NiterPaso*np.argmax(evoPa[2,:])))\n",
    "print(\"Resultado con validación (Test): %s\"%(evoPa[2,np.argmax(evoPa[1,:])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size=6, color=green> Entrenamiento de una red profunda\n",
    "<font size=4, color=blue> Se entrena en este caso un MLP de 4 capas ocultas y una capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fCoste='entropia' # Tipo de función de coste ('mmse', 'entropia' ,'wmmse')\n",
    "opt='gradiente'   # Tipo de optimizador ('gradiente', 'momento')\n",
    "tAct=[1,1,1,1,4]  # Tipos de activación de las neuronas de las capas oculta y de la capa de salida\n",
    "paso=[1e-3]       # Parametro de paso de la actualización por gradiente\n",
    "Nn1=125           # Número de neuronas de la primera capa oculta\n",
    "Nn2=125           # Número de neuronas de la segunda capa oculta\n",
    "Nn3=125           # Número de neuronas de la tercera capa oculta \n",
    "Nn4=125           # Número de neuronas de la cuarta capa oculta \n",
    "Niter=250         # Número de iteraciones para la actualización por gradiente\n",
    "Nbatch=250        # Número de muestras del mini-batch\n",
    "Nep=[Niter,Nbatch]\n",
    "\n",
    "(Np,Ne)=xTrainS.shape\n",
    "(Ns,Nada)=eTrainS.shape\n",
    "\n",
    "# Inicialización de los pesos de la capa oculta y de la capa de salida\n",
    "inicializacionExterna=False\n",
    "if inicializacionExterna:\n",
    "    wo1ini = 0.2*np.random.rand(Nn1,Ne+1)-0.1\n",
    "    wo2ini = 0.2*np.random.rand(Nn2,Nn1+1)-0.1\n",
    "    wo3ini = 0.2*np.random.rand(Nn3,Nn2+1)-0.1\n",
    "    wo4ini = 0.2*np.random.rand(Nn4,Nn3+1)-0.1\n",
    "    wsini = 0.2*np.random.rand(Ns,Nn4+1)-0.1\n",
    "    W=[wo1ini,wo2ini,wo3ini,wo4ini,wsini]\n",
    "else:\n",
    "    W=mdl.entrena_mlp(xTrainS,eTrainS,[Nn1,Nn2,Nn3,Nn4,[]],[0,0])[0]\n",
    "    wo1ini=W[0]\n",
    "    wo2ini=W[1]\n",
    "    wo3ini=W[2]\n",
    "    wo4ini=W[3]\n",
    "    wsini=W[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,Nep,fCoste=fCoste,optimizador=opt,tAct=tAct,paso=paso)[0:3]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del coste a minimizar', fontdict=font)\n",
    "plt.xlabel('Epoca', fontdict=font)\n",
    "plt.ylabel('Coste : %s' % (fCoste), fontdict=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eTrain=mdl.mlp(xTrainS,W,tAct)[0]\n",
    "eVal=mdl.mlp(xValD,W,tAct)[0]\n",
    "eTest=mdl.mlp(xTestD,W,tAct)[0]\n",
    "\n",
    "yTrainE=eTrain.argmax(axis=0)\n",
    "yValE=eVal.argmax(axis=0)\n",
    "yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "diferencia=yTrainE-yTrainS\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "diferencia=yValE-yVal\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "diferencia=yTestE-yTest\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "\n",
    "print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "print(\"Probabilidad de acierto (Test): %s\"%(PaTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representa la evolución de la tasa de acierto con las iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[wo1ini,wo2ini,wo3ini,wo4ini,wsini]\n",
    "NiterPaso=50\n",
    "NrepBucle=60\n",
    "evoPa=np.zeros((3,NrepBucle))\n",
    "for kbucle in range(NrepBucle):\n",
    "    (W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,[NiterPaso,Nbatch],fCoste=fCoste,optimizador=opt,tAct=tAct)[0:3]\n",
    "    (eTrain,xOculta)=mdl.mlp(xTrainS,W,tAct)\n",
    "    (eVal,xOculta)=mdl.mlp(xValD,W,tAct)\n",
    "    (eTest,xOculta)=mdl.mlp(xTestD,W,tAct)\n",
    "\n",
    "    yTrainE=eTrain.argmax(axis=0)\n",
    "    yValE=eVal.argmax(axis=0)\n",
    "    yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "    diferencia=yTrainE-yTrainS\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "    diferencia=yValE-yVal\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "    diferencia=yTestE-yTest\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "    PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "    evoPa[:,kbucle]=[PaTrain,PaVal,PaTest]\n",
    "    \n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "ejeIter=NiterPaso*np.linspace(1,NrepBucle,NrepBucle)\n",
    "plt.plot(ejeIter,evoPa[0,:],ejeIter,evoPa[1,:],ejeIter,evoPa[2,:])\n",
    "plt.title('Evolucion de la Prob. Acierto', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Prob Acierto (%)', fontdict=font)\n",
    "\n",
    "print(\"Entrenamiento (azul) - Validación (Naranja) - Test (verde)\")\n",
    "print(\" \")\n",
    "print(\"Max. Pa (Entrenamiento): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[0,:]),NiterPaso*np.argmax(evoPa[0,:])))\n",
    "print(\"Max. Pa (Validación): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[1,:]),NiterPaso*np.argmax(evoPa[1,:])))\n",
    "print(\"Max. Pa (Test) : %3.2f (Iter. %3.2f)\"%(np.max(evoPa[2,:]),NiterPaso*np.argmax(evoPa[2,:])))\n",
    "print(\"Resultado con validación (Test): %s\"%(evoPa[2,np.argmax(evoPa[1,:])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size=6, color=green>Entrenamiento de un autocodificador sin ruido\n",
    "\n",
    "<font size=4, color=blue>Se entrenará ahora un autocodificador para una red MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se inicializan los parámetros del codificador y de su entrenamiento\n",
    "tAct=[1,2]         # Tipos de activación de las neuronas de la capa oculta y de la capa de salida\n",
    "paso=[1e-3]        # Parametro de paso de la actualización por gradiente\n",
    "Nn=125             # Número de neuronas de la capa oculta \n",
    "Niter=250          # Número de iteraciones para la actualización por gradiente\n",
    "\n",
    "(Ne,Np)=xTrainS.shape\n",
    "(Ns,Nada)=eTrainS.shape\n",
    "# Inicialización de los pesos de la capa oculta y de la capa de salida\n",
    "weini = 0.2*np.random.rand(Nn,Ne+1)-0.1\n",
    "wrini = 0.2*np.random.rand(Ne,Nn+1)-0.1\n",
    "\n",
    "we1=weini\n",
    "wr1=wrini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(we1, wr1, coste, paso)=mdl.entrena_AE(xTrainS,we1,wr1,Niter,tAct,paso)\n",
    "# En realidad, un autocodificador sin ruido se puede ver como una red neuronal en la que la salida de referencia es\n",
    "# igual a los patrones de entrada\n",
    "#\n",
    "# W=[we1,wr1]\n",
    "# (W, paso, coste)=mdl.entrena_mlp(xTrainS,xTrainS,W,[Niter,0],tAct=tAct,paso=paso)[0:3]\n",
    "# we1=W[0]\n",
    "# wr1=W[1]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del MSE para codificador sin ruido', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('MSE', fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se representan las reconstrucciones obtenidas a partir del codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(xTrainDrec,xTrainE1)=mdl.mlp(xTrainD,[we1,wr1],tAct)\n",
    "\n",
    "indices=[]\n",
    "\n",
    "for k in range(10):\n",
    "    indNum = [i for (i, val) in enumerate(yTrain[0:30]) if val == int(k)]\n",
    "    indices.append(indNum[0])\n",
    "\n",
    "figA, ejesA = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figA.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "figB, ejesB = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figB.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejesA.flat, indices):\n",
    "    imagen = xTrainD[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    etiqueta = yTrain[k]\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    eje.set_title('a')\n",
    "    eje.set_title('{label}'.format(label=etiqueta))\n",
    "for eje, k in zip(ejesB.flat, indices):    \n",
    "    imagen = xTrainDrec[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se representan los pesos de algunas neuronas de la capa oculta (codificador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indice=0\n",
    "imagen = we1[indice,0:Ne]\n",
    "resI=int(np.sqrt(Ne))\n",
    "imagen=imagen.reshape(resI,resI)\n",
    "fig, eje = plt.subplots(figsize=(6, 6))\n",
    "eje.set_title('Pesos de la neurona {label} (AE)'.format(label=indice+1))\n",
    "eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=range(100)\n",
    "resI=int(np.sqrt(Ne))\n",
    "\n",
    "fig, ejes = plt.subplots(5, 20, figsize=(19, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejes.flat, indices):\n",
    "    imagen = we1[k,0:Ne]\n",
    "    imagen=imagen.reshape(resI,resI)\n",
    "    eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=6, color=green>Entrenamiento de un autocodificador con ruido\n",
    "<font size=4, color=blue> Ahora se entrena un autocodificador con ruido en la entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se inicializan los parámetros del codificador y de su entrenamiento\n",
    "tAct=[1,2]       # Tipos de activación de las neuronas de la capa oculta y de la capa de salida\n",
    "paso=[1e-3]      # Parametro de paso de la actualización por gradiente\n",
    "Nn=125           # Número de neuronas de la capa oculta \n",
    "Niter=250        # Número de iteraciones para la actualización por gradiente\n",
    "pRuido=0.25      # Probabilidad de poner a cero una cierta entrada\n",
    "\n",
    "# Inicialización de los pesos de la capa oculta y de la capa de salida\n",
    "# weini = 0.2*np.random.rand(Nn,Ne+1)-0.1\n",
    "# wdini = 0.2*np.random.rand(Ne,Nn+1)-0.1\n",
    "\n",
    "we1n=weini\n",
    "wr1n=wrini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(we1n, wr1n, coste, paso)=mdl.entrena_DAE(xTrainS,we1n,wr1n,Niter,tAct,paso,pRuido)\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del MSE para un codificador con ruido', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('MSE', fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se representan las reconstrucciones obtenidas a partir del codificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrainDrec,xTrainE1n)=mdl.mlp(xTrainD,[we1n,wr1n],tAct)\n",
    "\n",
    "indices=[]\n",
    "\n",
    "for k in range(10):\n",
    "    indNum = [i for (i, val) in enumerate(yTrain[0:30]) if val == int(k)]\n",
    "    indices.append(indNum[0])\n",
    "\n",
    "figA, ejesA = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figA.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "figB, ejesB = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figB.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejesA.flat, indices):\n",
    "    imagen = xTrainD[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    etiqueta = yTrain[k]\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    eje.set_title('a')\n",
    "    eje.set_title('{label}'.format(label=etiqueta))\n",
    "for eje, k in zip(ejesB.flat, indices):    \n",
    "    imagen = xTrainDrec[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se representan los pesos de algunas neuronas de la capa oculta (codificador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice=0\n",
    "imagen = we1n[indice,0:Ne]\n",
    "resI=int(np.sqrt(Ne))\n",
    "imagen=imagen.reshape(resI,resI)\n",
    "fig, eje = plt.subplots(figsize=(6, 6))\n",
    "eje.set_title('Pesos de la neurona {label} (DAE)'.format(label=indice+1))\n",
    "eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=range(100)\n",
    "resI=int(np.sqrt(Ne))\n",
    "\n",
    "fig, ejes = plt.subplots(5, 20, figsize=(19, 5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejes.flat, indices):\n",
    "    imagen = we1n[k,0:Ne]\n",
    "    imagen=imagen.reshape(resI,resI)\n",
    "    eje.imshow(imagen, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size=6, color=green>Autocodificadores \"apilados\"\n",
    "<font size=4, color=blue>Se utilizarán ahora autoencoders para obtener los parámetros de las capas ocultas de una red profunda. A esta alternativa se le denomina autocodificadores apilados, o en inglés (Stacked Denoising Autoencoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se comienza por definir los parámetros básicos del sistema\n",
    "tActAE1=[1,2]     # Tipos de activación de las neuronas de la capas oculta y de la capa de salida del AE-1\n",
    "tActAE2=[1,1]     # Tipos de activación de las neuronas de la capas oculta y de la capa de salida del AE-2\n",
    "tActAE3=[1,1]     # Tipos de activación de las neuronas de la capas oculta y de la capa de salida del AE-3\n",
    "paso=[1e-3]       # Parametro de paso de la actualización por gradiente\n",
    "Nn1=125           # Número de neuronas de la primera capa oculta\n",
    "Nn2=125           # Número de neuronas de la segunda capa oculta\n",
    "Nn3=125           # Número de neuronas de la tercera capa oculta \n",
    "Niter=250         # Número de iteraciones para la actualización por gradiente\n",
    "pRuido=0.25       # Probabilidad de anular una de las entradas\n",
    "\n",
    "(Ne,Np)=xTrainS.shape\n",
    "# Inicialización de los pesos de los autocodificadores\n",
    "we1ini = 0.2*np.random.rand(Nn1,Ne+1)-0.1\n",
    "we2ini = 0.2*np.random.rand(Nn2,Nn1+1)-0.1\n",
    "we3ini = 0.2*np.random.rand(Nn3,Nn2+1)-0.1\n",
    "# Inicialización de los pesos de reconstrucción\n",
    "wr1ini = 0.2*np.random.rand(Ne,Nn1+1)-0.1\n",
    "wr2ini = 0.2*np.random.rand(Nn1,Nn2+1)-0.1\n",
    "wr3ini = 0.2*np.random.rand(Nn2,Nn3+1)-0.1\n",
    "# Inicialización de los pesos\n",
    "we1=we1ini\n",
    "we2=we2ini\n",
    "we3=we3ini\n",
    "wr1=wr1ini\n",
    "wr2=wr2ini\n",
    "wr3=wr3ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se entrena el primer AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(we1, wr1, coste, paso)=mdl.entrena_DAE(xTrainS,we1,wr1,Niter,tActAE1,paso,pRuido)\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del MSE para el primer AE', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('MSE', fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se realiza la proyección obtenida con el primer autocodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainSE1=mdl.mlp(xTrainS,[we1,wr1],tActAE1)[1][0]\n",
    "xTestDE1=mdl.mlp(xTestD,[we1,wr1],tActAE1)[1][0]\n",
    "xValDE1=mdl.mlp(xValD,[we1,wr1],tActAE1)[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se entrena el segundo AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(we2, wr2, coste, paso)=mdl.entrena_DAE(xTrainSE1,we2,wr2,Niter,tActAE2,paso,pRuido)\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del MSE para el segundo AE', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('MSE', fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se realiza la proyección obtenida con el segundo autocodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainSE2=mdl.mlp(xTrainSE1,[we2,wr2],tActAE2)[1][0]\n",
    "xTestDE2=mdl.mlp(xTestDE1,[we2,wr2],tActAE2)[1][0]\n",
    "xValDE2=mdl.mlp(xValDE1,[we2,wr2],tActAE2)[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se entrena el tercer AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(we3, wr3, coste, paso)=mdl.entrena_DAE(xTrainSE2,we3,wr3,Niter,tActAE3,paso,pRuido)\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del MSE para el tercer AE', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('MSE', fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se realiza la proyección obtenida con el tercer autocodificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrainSE3=mdl.mlp(xTrainSE2,[we3,wr3],tActAE3)[1][0]\n",
    "xTestDE3=mdl.mlp(xTestDE2,[we3,wr3],tActAE3)[1][0]\n",
    "xValDE3=mdl.mlp(xValDE2,[we3,wr3],tActAE3)[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se entrena ahora la etapa de salida (una capa oculta y una de salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fCoste='entropia'   # Tipo de función de coste ('mmse', 'entropia' ,'wmmse')\n",
    "opt='gradiente'     # Tipo de optimizador ('gradiente', 'momento')\n",
    "tActFin=[1,4]       # Tipos de activación de las neuronas de la capa oculta y de la capa de salida\n",
    "paso=1e-3           # Parametro de paso de la actualización por gradiente\n",
    "NnFin=150           # Número de neuronas de la capa oculta \n",
    "Niter=250           # Número de iteraciones para la actualización por gradiente\n",
    "Nbatch=250            # Número de patrones del mini-batch \n",
    "Nep=[Niter,Nbatch]\n",
    "\n",
    "(Ne,Np)=xTrainSE3.shape\n",
    "(Ns,Nada)=eTrainS.shape\n",
    "\n",
    "# Inicialización de los pesos de la capa oculta y de la capa de salida\n",
    "inicializacionExterna=True\n",
    "if inicializacionExterna:\n",
    "    woini = 0.2*np.random.rand(NnFin,Ne+1)-0.1\n",
    "    wsini = 0.2*np.random.rand(Ns,NnFin+1)-0.1\n",
    "    Wfin=[woini,wsini]\n",
    "else:\n",
    "    Wfin=mdl.entrena_mlp(xTrainS,eTrainS,[NnFin,[]],[0,0])[0]\n",
    "    woini=Wfin[0]\n",
    "    wsini=Wfin[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Niter=3000\n",
    "#Nep=[Niter,250]\n",
    "(Wfin, paso, coste) = mdl.entrena_mlp(xTrainSE3,eTrainS,Wfin,Nep,fCoste=fCoste,optimizador=opt,tAct=tActFin)[0:3]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del coste a minimizar', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Coste : %s' % (fCoste), fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se estiman las prestaciones obtenidas con esta etapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eTrain=mdl.mlp(xTrainSE3,Wfin,tActFin)[0]\n",
    "eVal=mdl.mlp(xValDE3,Wfin,tActFin)[0]\n",
    "eTest=mdl.mlp(xTestDE3,Wfin,tActFin)[0]\n",
    "\n",
    "yTrainE=eTrain.argmax(axis=0)\n",
    "yValE=eVal.argmax(axis=0)\n",
    "yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "diferencia=yTrainE-yTrainS\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "diferencia=yValE-yVal\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "diferencia=yTestE-yTest\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "\n",
    "print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "print(\"Probabilidad de acierto (Test): %s\"%(PaTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Se realiza el ajuste fino sobre la red global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se agrupan en una lista los pesos de cada capa y las funciones de activación de cada capa\n",
    "Wtotal=[we1,we2,we3,Wfin[0],Wfin[1]] \n",
    "paso=[1e-3]\n",
    "tActTotal=np.array([tActAE1[0], tActAE2[0], tActAE3[0], tActFin[0], tActFin[1]])\n",
    "Niter=50            # Número de iteraciones para la actualización por gradiente\n",
    "Nbatch=0            # Número de patrones del mini-batch \n",
    "Nep=[Niter,Nbatch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entrena la red global con las 4 capas ocultas y 1 capa de salida\n",
    "Niter=500\n",
    "Nep=[Niter,250]\n",
    "(Wtotal, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,Wtotal,Nep,fCoste=fCoste,\n",
    "                                        optimizador=opt,tAct=tActTotal,paso=paso)[0:3]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del coste a minimizar', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Coste : %s' % (fCoste), fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se estiman las prestaciones obtenidas con la red completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTrain=mdl.mlp(xTrainS,Wtotal,tActTotal)[0]\n",
    "eVal=mdl.mlp(xValD,Wtotal,tActTotal)[0]\n",
    "eTest=mdl.mlp(xTestD,Wtotal,tActTotal)[0]\n",
    "\n",
    "yTrainE=eTrain.argmax(axis=0)\n",
    "yValE=eVal.argmax(axis=0)\n",
    "yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "diferencia=yTrainE-yTrainS\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "diferencia=yValE-yVal\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "diferencia=yTestE-yTest\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "\n",
    "print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "print(\"Probabilidad de acierto (Test): %s\"%(PaTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Imágenes reconstruidas a partir de las representaciones obtenidas por los distintos autocodificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La reconstrucción del primer autocodificador se podría obtener directamente\n",
    "#(xTrainDrec1,xTrainDE1)=mdl.mlp(xTrainS,[we1,wr1],tActAE1)\n",
    "#xTrainDE1=xTrainDE1[0]\n",
    "xTrainDE1=mdl.mlp(xTrainD,[we1,wr1],tActAE1)[1][0]\n",
    "xTrainDE2=mdl.mlp(xTrainDE1,[we2,wr2],tActAE2)[1][0]\n",
    "xTrainDE3=mdl.mlp(xTrainDE2,[we3,wr3],tActAE3)[1][0]\n",
    "\n",
    "xTrainDrec1=mdl.mlp(xTrainDE1,[wr1],[tActAE1[1]])[0]\n",
    "xTrainDrec2=mdl.mlp(xTrainDE2,[wr2,wr1],[tActAE2[1],tActAE1[1]])[0]\n",
    "xTrainDrec3=mdl.mlp(xTrainDE3,[wr3,wr2,wr1],[tActAE3[1],tActAE2[1],tActAE1[1]])[0]\n",
    "\n",
    "indices=[]\n",
    "\n",
    "for k in range(10):\n",
    "    indNum = [i for (i, val) in enumerate(yTrain[0:30]) if val == int(k)]\n",
    "    indices.append(indNum[0])\n",
    "\n",
    "figA, ejesA = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figA.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "figB, ejesB = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figB.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "figC, ejesC = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figC.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "figD, ejesD = plt.subplots(1, 10, figsize=(12, 3), subplot_kw={'xticks': [], 'yticks': []})\n",
    "figD.subplots_adjust(hspace=0.3, wspace=0.05)\n",
    "\n",
    "for eje, k in zip(ejesA.flat, indices):\n",
    "    imagen = xTrainD[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    etiqueta = yTrain[k]\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    eje.set_title('a')\n",
    "    eje.set_title('{label}'.format(label=etiqueta))\n",
    "    \n",
    "for eje, k in zip(ejesB.flat, indices):    \n",
    "    imagen = xTrainDrec1[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    eje.imshow(imagen, cmap='gray') \n",
    "    \n",
    "for eje, k in zip(ejesC.flat, indices):    \n",
    "    imagen = xTrainDrec2[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    eje.imshow(imagen, cmap='gray')\n",
    "    \n",
    "for eje, k in zip(ejesD.flat, indices):    \n",
    "    imagen = xTrainDrec3[:,k]\n",
    "    imagen=imagen.reshape(len(pixeles),len(pixeles))\n",
    "    eje.imshow(imagen, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=6, color=green>Red neuronal profunda con Drop-Out\n",
    "<font size=4, color=blue>Se entrenará ahora una red neuronal de varias capas ocultas con inicialización aleatoria, Drop-Out, y funciones de activación ReLU (Rectified Linear Units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se comienza por definir los parámetros básicos del sistema\n",
    "fCoste='entropia'        # Tipo de función de coste ('mmse', 'entropia' ,'wmmse')\n",
    "opt='gradiente'          # Tipo de optimizador ('gradiente', 'momento')\n",
    "tAct=[3,3,3,3,4]         # Tipos de activación de las neuronas de las capas oculta y de la capa de salida\n",
    "paso=[1e-3]              # Parametro de paso de la actualización por gradiente\n",
    "Nn1=125                  # Número de neuronas de la primera capa oculta\n",
    "Nn2=125                  # Número de neuronas de la segunda capa oculta\n",
    "Nn3=125                  # Número de neuronas de la tercera capa oculta \n",
    "Nn4=125                  # Número de neuronas de la tercera capa oculta \n",
    "Niter=500                # Número de iteraciones para la actualización por gradiente\n",
    "Nbatch=250               # Número de muestras del mini-batch \n",
    "Nep=[Niter,Nbatch]\n",
    "pDO=[0.1,0.3,0.3,0.3,0.3]  # Probabilidades de Drop-Out (de la entrada y de las capas ocultas)\n",
    "\n",
    "# Se inicializan los pesos\n",
    "Wini = mdl.entrena_mlp(xTrainS,eTrainS,[Nn1,Nn2,Nn3,Nn4,[]],[0,0])[0]\n",
    "W=list(Wini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Niter=2000\n",
    "#Nep=[Niter,250]\n",
    "(W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,Nep,fCoste=fCoste,\n",
    "                                   optimizador=opt,tAct=tAct,paso=paso,pDO=pDO)[0:3]\n",
    "\n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "plt.plot(range(Niter+1),coste)\n",
    "plt.title('Evolucion del coste a minimizar', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Coste : %s' % (fCoste), fontdict=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se estiman las prestaciones obtenidas con esta red (con Drop-Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTrain=mdl.mlp(xTrainS,W,tAct)[0]\n",
    "eVal=mdl.mlp(xValD,W,tAct)[0]\n",
    "eTest=mdl.mlp(xTestD,W,tAct)[0]\n",
    "\n",
    "yTrainE=eTrain.argmax(axis=0)\n",
    "yValE=eVal.argmax(axis=0)\n",
    "yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "diferencia=yTrainE-yTrainS\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "diferencia=yValE-yVal\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "diferencia=yTestE-yTest\n",
    "indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "\n",
    "print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "print(\"Probabilidad de acierto (Test): %s\"%(PaTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue> Se representa la evolución de la tasa de acierto con las iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=list(Wini)\n",
    "pEnt=0.1\n",
    "pOcu=0.25\n",
    "pDO=[pEnt,pOcu,pOcu,pOcu,pOcu]\n",
    "NiterPaso=50\n",
    "NrepBucle=100\n",
    "evoPa=np.zeros((3,NrepBucle))\n",
    "for kbucle in range(NrepBucle):\n",
    "    #(W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,[NiterPaso,Nbatch],fCoste=fCoste,optimizador=opt,tAct=tAct)[0:3]\n",
    "    (W, paso, coste) = mdl.entrena_mlp(xTrainS,eTrainS,W,[NiterPaso,Nbatch],fCoste=fCoste,\n",
    "                                   optimizador=opt,tAct=tAct,paso=paso,pDO=pDO)[0:3]\n",
    "    (eTrain,xOculta)=mdl.mlp(xTrainS,W,tAct)\n",
    "    (eVal,xOculta)=mdl.mlp(xValD,W,tAct)\n",
    "    (eTest,xOculta)=mdl.mlp(xTestD,W,tAct)\n",
    "\n",
    "    yTrainE=eTrain.argmax(axis=0)\n",
    "    yValE=eVal.argmax(axis=0)\n",
    "    yTestE=eTest.argmax(axis=0)\n",
    "\n",
    "    diferencia=yTrainE-yTrainS\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "\n",
    "    diferencia=yValE-yVal\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "\n",
    "    diferencia=yTestE-yTest\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "    PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "    evoPa[:,kbucle]=[PaTrain,PaVal,PaTest]\n",
    "    \n",
    "font = {'family': 'serif','color':  'darkred','weight': 'normal','size': 16}\n",
    "ejeIter=NiterPaso*np.linspace(1,NrepBucle,NrepBucle)\n",
    "plt.plot(ejeIter,evoPa[0,:],ejeIter,evoPa[1,:],ejeIter,evoPa[2,:])\n",
    "plt.title('Evolucion de la Prob. Acierto', fontdict=font)\n",
    "plt.xlabel('Iteracion', fontdict=font)\n",
    "plt.ylabel('Prob Acierto (%)', fontdict=font)\n",
    "\n",
    "print(\"Entrenamiento (azul) - Validación (Naranja) - Test (verde)\")\n",
    "print(\" \")\n",
    "print(\"Max. Pa (Entrenamiento): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[0,:]),NiterPaso*np.argmax(evoPa[0,:])))\n",
    "print(\"Max. Pa (Validación): %3.2f (Iter. %3.2f)\"%(np.max(evoPa[1,:]),NiterPaso*np.argmax(evoPa[1,:])))\n",
    "print(\"Max. Pa (Test) : %3.2f (Iter. %3.2f)\"%(np.max(evoPa[2,:]),NiterPaso*np.argmax(evoPa[2,:])))\n",
    "print(\"Resultado con validación (Test): %s\"%(evoPa[2,np.argmax(evoPa[1,:])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=6, color=green>Red neuronal convolucional (CNN)\n",
    "<font size=4, color=blue>Se entrenará ahora una red neuronal convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Definición de la función para el entrenamiento de la red CNN usando TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la función básica para el entrenamiento de la red CNN usando TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def entrenaConv(xTrain,yTrain,W,Nepoch=[500,250],paso=1e-3,pDO=0.2,\n",
    "                Npix=28,Npix2=7,Nconv1=5,Nconv2=5,Nfilt1=16,Nfilt2=32,\n",
    "                Nneu=1024,padding=['SAME','SAME']):\n",
    "    \n",
    "    Niter = Nepoch[0]\n",
    "    Nbatch = Nepoch[1]               \n",
    "    (Ne,Np)=np.shape(xTrain)\n",
    "    (Ns,Np)=np.shape(yTrain)\n",
    "    ordenBatch = np.random.permutation(Np)\n",
    "    Nini=0\n",
    "    pDOkeep=1-pDO\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Construcción de la red MLP Convolucional con Drop-out - Tensores              \n",
    "    #--------------------------------------------------------------------------\n",
    "    x = tf.placeholder(tf.float32, shape=[None, Ne])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, Ns])\n",
    "    DO_keep_prob = tf.placeholder(tf.float32)\n",
    "    pasoT = tf.placeholder(tf.float32)\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Inicialización de pesos\n",
    "    #--------------------------------------------------------------------------\n",
    "    x_image = tf.reshape(x, [-1, Npix, Npix, 1])\n",
    "    \n",
    "    W_conv1 = weight_variable([Nconv1, Nconv1, 1, Nfilt1])\n",
    "    b_conv1 = bias_variable([Nfilt1])\n",
    "    \n",
    "    W_conv2 = weight_variable([Nconv2, Nconv2, Nfilt1, Nfilt2])\n",
    "    b_conv2 = bias_variable([Nfilt2])\n",
    "    \n",
    "    W_fc1 = weight_variable([Npix2 * Npix2 * Nfilt2, Nneu])\n",
    "    b_fc1 = bias_variable([Nneu])    \n",
    "           \n",
    "    W_fc2 = weight_variable([Nneu, Ns])\n",
    "    b_fc2 = bias_variable([Ns])    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Se propaga la red\n",
    "    #--------------------------------------------------------------------------\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, \n",
    "                        strides=[1, 1, 1, 1], padding=padding[0])+b_conv1)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding=padding[0])\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, \n",
    "                        strides=[1, 1, 1, 1], padding=padding[1])+b_conv2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding=padding[1])\n",
    "        \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, Npix2*Npix2*Nfilt2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)        \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, DO_keep_prob)\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "                   \n",
    "    coste_CONV = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_conv))\n",
    "    #coste_CONV = tf.reduce_mean(tf.squared_difference(y,y_conv))\n",
    "          \n",
    "    entreno = tf.train.AdamOptimizer(learning_rate = pasoT).minimize(coste_CONV)     \n",
    "    #----------------------------------------------------------------------------\n",
    "    # Training - Se lanza la sesión\n",
    "    #----------------------------------------------------------------------------\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if len(W)>0:\n",
    "        Wa = tf.assign(W_conv1,W[0])\n",
    "        Wb = tf.assign(b_conv1,W[1])\n",
    "        Wc = tf.assign(W_conv2,W[2])\n",
    "        Wd = tf.assign(b_conv2,W[3])\n",
    "        We = tf.assign(W_fc1,W[4])\n",
    "        Wf = tf.assign(b_fc1,W[5])\n",
    "        Wg = tf.assign(W_fc2,W[6])\n",
    "        Wh = tf.assign(b_fc2,W[7])\n",
    "        sess.run([Wa,Wb,Wc,Wd])\n",
    "        sess.run([We,Wf,Wg,Wh])\n",
    "            \n",
    "    # Training loop\n",
    "    for epoch in range(Niter+1):\n",
    "        if (Nini+Nbatch) > Np:\n",
    "            ordenBatch = np.random.permutation(Np)        \n",
    "            indBatch=ordenBatch[np.arange(Nbatch)]\n",
    "            Nini=Nbatch\n",
    "        else:        \n",
    "            indBatch=ordenBatch[Nini+np.arange(Nbatch)]\n",
    "            Nini=Nini+Nbatch\n",
    "            \n",
    "        xTrainBatch=xTrain[:,indBatch]\n",
    "        yTrainBatch=yTrain[:,indBatch]\n",
    "\n",
    "        # Entreno con los datos del BATCH\n",
    "        sess.run(entreno, feed_dict={x: xTrainBatch.T, y: yTrainBatch.T, pasoT: paso, DO_keep_prob: pDOkeep})        \n",
    "\n",
    "    \n",
    "    (a,b,c,d)=sess.run([W_conv1,b_conv1,W_conv2,b_conv2])\n",
    "    (e,f,g,h)=sess.run([W_fc1,b_fc1,W_fc2,b_fc2])\n",
    "    Wtodo=[a,b,c,d,e,f,g,h]\n",
    "\n",
    "    sess.close()\n",
    "    pRed=[Npix,Npix2,Nconv1,Nconv2,Nfilt1,Nfilt2,Nneu,Ns,padding]\n",
    "    return (Wtodo,pRed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Definición de la función evaluar la salida de la red CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluaConv(xTrain,Wtodo,pRed=[28,7,5,5,32,64,1024,10,['SAME','SAME']]):\n",
    "    \n",
    "    (Ne,Np)=np.shape(xTrain)\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Tensores y definición de parámetros básicos\n",
    "    #--------------------------------------------------------------------------\n",
    "    x = tf.placeholder(tf.float32, shape=[None, Ne])\n",
    "    DO_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    Npix=pRed[0]\n",
    "    Npix2=pRed[1]\n",
    "    Nconv1=pRed[2]\n",
    "    Nconv2=pRed[3]\n",
    "    Nfilt1=pRed[4]\n",
    "    Nfilt2=pRed[5]   \n",
    "    Nneu=pRed[6]\n",
    "    Ns=pRed[7]\n",
    "    padding=pRed[8]\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Inicialización de pesos\n",
    "    #--------------------------------------------------------------------------\n",
    "    x_image = tf.reshape(x, [-1, Npix, Npix, 1])\n",
    "    \n",
    "    W_conv1 = weight_variable([Nconv1, Nconv1, 1, Nfilt1])\n",
    "    b_conv1 = bias_variable([Nfilt1])\n",
    "    \n",
    "    W_conv2 = weight_variable([Nconv2, Nconv2, Nfilt1, Nfilt2])\n",
    "    b_conv2 = bias_variable([Nfilt2])\n",
    "    \n",
    "    W_fc1 = weight_variable([Npix2 * Npix2 * Nfilt2, Nneu])\n",
    "    b_fc1 = bias_variable([Nneu])    \n",
    "           \n",
    "    W_fc2 = weight_variable([Nneu, Ns])\n",
    "    b_fc2 = bias_variable([Ns])    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # Se propaga la red\n",
    "    #--------------------------------------------------------------------------\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, \n",
    "                        strides=[1, 1, 1, 1], padding=padding[0])+b_conv1)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding=padding[0])\n",
    "    h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, \n",
    "                        strides=[1, 1, 1, 1], padding=padding[1])+b_conv2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding=padding[1])\n",
    "        \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, Npix2*Npix2*Nfilt2])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)        \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, DO_keep_prob)\n",
    "    \n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2                   \n",
    "    #------------------------------------------------------------------------------\n",
    "    # Evaluación de la salida de la red - Se lanza la sesión\n",
    "    #------------------------------------------------------------------------------\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    Wa = tf.assign(W_conv1,Wtodo[0])\n",
    "    Wb = tf.assign(b_conv1,Wtodo[1])\n",
    "    Wc = tf.assign(W_conv2,Wtodo[2])\n",
    "    Wd = tf.assign(b_conv2,Wtodo[3])\n",
    "    We = tf.assign(W_fc1,Wtodo[4])\n",
    "    Wf = tf.assign(b_fc1,Wtodo[5])\n",
    "    Wg = tf.assign(W_fc2,Wtodo[6])\n",
    "    Wh = tf.assign(b_fc2,Wtodo[7])\n",
    "    sess.run([Wa,Wb,Wc,Wd])\n",
    "    sess.run([We,Wf,Wg,Wh])    \n",
    "        \n",
    "    eTrain = sess.run(y_conv, feed_dict={x: xTrain.T, DO_keep_prob: 1.0})\n",
    "\n",
    "    sess.close()\n",
    "    return (eTrain.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Definición de los parámetros básicos de la red CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wcnn=[]        # Inicialización de pesos de la CNN \n",
    "Niter=25       # Número de iteraciones de cada paso \n",
    "Nbatch=250     # Número de muestras de cada mini-batch\n",
    "Nreps=50       # Número de pasos de entrenamiento\n",
    "pDO=0.2\n",
    "evoRes=np.zeros((3,Nreps))\n",
    "if xTrainS.shape[0]==100: # 10x10 píxeles\n",
    "    Npix=10    # Número de píxeles por dirección        \n",
    "    Nconv1=3   # Tamaño del filtro convolucional 1 (píxeles)\n",
    "    Nconv2=3   # Tamaño del filtro convolucional 1 (píxeles) \n",
    "    padding=['VALID','SAME']  # Tipo de padding para las dos etapas\n",
    "    Npix2=2    # Número de píxeles tras las dos etapas de Pooling\n",
    "    Nf1=16     # Número de filtros de la primera etapa\n",
    "    Nf2=32     # Número de filtros de la segunda etapa\n",
    "    Nn=256     # Número de neuronas del MLP final\n",
    "elif xTrainS.shape[0]==196: # 14x14 píxeles\n",
    "    Npix=14    # Número de píxeles por dirección        \n",
    "    Nconv1=3   # Tamaño del filtro convolucional 1 (píxeles)\n",
    "    Nconv2=3   # Tamaño del filtro convolucional 1 (píxeles) \n",
    "    padding=['VALID','SAME']  # Tipo de padding para las dos etapas\n",
    "    Npix2=3    # Número de píxeles tras las dos etapas de Pooling\n",
    "    Nf1=32     # Número de filtros de la primera etapa\n",
    "    Nf2=64     # Número de filtros de la segunda etapa\n",
    "    Nn=1024    # Número de neuronas del MLP final\n",
    "elif xTrainS.shape[0]==784: # 18x28 píxeles \n",
    "    Npix=28    # Número de píxeles por dirección        \n",
    "    Nconv1=5   # Tamaño del filtro convolucional 1 (píxeles)\n",
    "    Nconv2=5   # Tamaño del filtro convolucional 1 (píxeles) \n",
    "    padding=['SAME','SAME']  # Tipo de padding para las dos etapas\n",
    "    Npix2=7    # Número de píxeles tras las dos etapas de Pooling\n",
    "    Nf1=32     # Número de filtros de la primera etapa\n",
    "    Nf2=64     # Número de filtros de la segunda etapa\n",
    "    Nn=1024    # Número de neuronas del MLP final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4, color=blue>Entrenamiento iterativo de la red CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kRep in range(Nreps):\n",
    "    (Wcnn,pRed)=entrenaConv(xTrainS,eTrainS,Wcnn,Nepoch=[Niter,Nbatch],paso=1e-3,\n",
    "                 pDO=pDO,padding=padding,Npix=Npix,Npix2=Npix2,Nconv1=Nconv1,Nconv2=Nconv2,\n",
    "                 Nfilt1=Nf1,Nfilt2=Nf2,Nneu=Nn)\n",
    "        \n",
    "    eTrain=evaluaConv(xTrainS,Wcnn,pRed)\n",
    "    eVal=evaluaConv(xValD,Wcnn,pRed)\n",
    "    eTest=evaluaConv(xTestD,Wcnn,pRed)\n",
    "        \n",
    "    yTrainE=eTrain.argmax(axis=0)\n",
    "    yValE=eVal.argmax(axis=0)\n",
    "    yTestE=eTest.argmax(axis=0)\n",
    "        \n",
    "    diferencia=yTrainE-yTrainS\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaTrain=(100.0*len(indAcierto))/len(yTrainS)\n",
    "        \n",
    "    diferencia=yValE-yVal\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    PaVal=(100.0*len(indAcierto))/len(yVal)\n",
    "        \n",
    "    diferencia=yTestE-yTest\n",
    "    indAcierto = [i for (i, val) in enumerate(diferencia) if val == 0]\n",
    "    indFallo = [i for (i, val) in enumerate(diferencia) if val != 0]\n",
    "    PaTest=(100.0*len(indAcierto))/len(yTest)\n",
    "        \n",
    "    evoRes[:,kRep]=[PaTrain,PaVal,PaTest]\n",
    "        \n",
    "    print(\"Repetición %d de %d\"%(kRep+1,Nreps))\n",
    "    print(\"Probabilidad de acierto (Entrenamiento): %s\"%(PaTrain))\n",
    "    print(\"Probabilidad de acierto (Validación): %s\"%(PaVal))\n",
    "    print(\"Probabilidad de acierto (Test): %s\"%(PaTest))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl=reload(mdl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
